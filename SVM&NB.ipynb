{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Code: DA-AG-013\n",
        "# SVM & Naive Bayes | Assignment\n",
        "\n"
      ],
      "metadata": {
        "id": "-HnWyiDYBX3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:  What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "Ans> Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It tries to find the best boundary known as hyperplane that separates different classes in the data. It is useful when you want to do binary classification like spam vs. not spam or cat vs. dog.\n",
        "\n",
        "The key idea behind the SVM algorithm is to find the hyperplane that best separates two classes by maximizing the margin between them. This margin is the distance from the hyperplane to the nearest data points (support vectors) on each side.The best hyperplane also known as the \"hard margin\" is the one that maximizes the distance between the hyperplane and the nearest data points from both classes."
      ],
      "metadata": {
        "id": "Xqzq-Uhybf0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "Ans>\n",
        "**Hard Margin SVM**:\n",
        "*  Attempts to find a hyperplane that perfectly separates the classes without any misclassification.\n",
        "* Suitable when datasets are linearly separable,not robust to noise or overlapping classes.\n",
        "* Maximizes the margin strictly, no data points are allowed inside or on the wrong side of the margin.\n",
        "* Very sensitive to noise; a single misclassified point can make the optimization infeasible.\n",
        "* No explicit hyperparameter for error trade-off; it assumes perfect separability.\n",
        "* All points are strictly outside the margin boundaries; the hyperplane touches the closest points (support vectors).\n",
        "\n",
        "**Soft Margin SVM**:\n",
        "*  Allows for some misclassifications to achieve a better trade-off between margin width and classification error.\n",
        "* uitable for non-linearly separable or noisy datasets, introduces flexibility to handle overlaps and outliers.\n",
        "* Maximizes effective margin while permitting some points to violate the margin to reduce overall error.\n",
        "* Hyperparameter must be tuned to balance margin width vs. error tolerance\n",
        "* Some points may lie within the margin or be misclassified, but the hyperplane is chosen to optimize overall generalization."
      ],
      "metadata": {
        "id": "3UUBonlSbfZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "\n",
        "Ans> The kernel trick is a method used in SVMs to enable them to classify non-linear data using a linear classifier. By applying a kernel function, SVMs can implicitly map input data into a higher-dimensional space where a linear separator (hyperplane) can be used to divide the classes.\n",
        "\n",
        "**example:** This kernel allows SVM to model non-linear relationships by implicitly computing all polynomial combinations of input features up to degree, without explicitly creating new features.\n",
        "\n",
        "**Its use:** The polynomial kernel is particularly useful when data exhibits curved or interaction effects between features. For example:\n",
        "In a 2D classification problem where data points form a parabolic or checkerboard pattern, the polynomial kernel can map it into a higher-dimensional space where these patterns become separable by a hyperplane."
      ],
      "metadata": {
        "id": "4n3NdpRObfmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve” ?\n",
        "\n",
        "Ans> The Naive Bayes Classifier is a simple probabilistic classifier and it has very few number of parameters which are used to build the ML models that can predict at a faster speed than other classification algorithms.\n",
        "It is a probabilistic classifier because it assumes that one feature in the model is independent of existence of another feature. In other words, each feature contributes to the predictions with no relation between each other.\n",
        "\n",
        "It is named as \"Naive\" because it assumes the presence of one feature does not affect other features."
      ],
      "metadata": {
        "id": "282tZOYdbfvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?\n",
        "\n",
        "Ans>\n",
        "1. Gaussian Naive Bayes: It is continuous values associated with each feature are assumed to be distributed according to a Gaussian distribution.\n",
        "\n",
        "use: Ideal when your dataset consists of numerical features such as height, weight, temperature, or blood pressure.\n",
        "Doesn’t require feature scaling (like normalization or standardization) due to its probabilistic nature.\n",
        "2. Multinomial Naive Bayes: It is used when features represent the frequency of terms (such as word counts) in a document. It is commonly applied in text classification, where term frequencies are important.\n",
        "\n",
        "use:Primarily used in text classification where features are word counts or TF-IDF values.\n",
        "Works well with bag-of-words or n-gram representations.\n",
        "3. Bernoulli: It deals with binary features, where each feature indicates whether a word appears or not in a document. It is suited for scenarios where the presence or absence of terms is more relevant than their frequency. Both models are widely used in document classification tasks.\n",
        "use: Appropriate for datasets where you’re interested in whether a feature appears, rather than how often."
      ],
      "metadata": {
        "id": "urqtvwy3n_YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6:   Write a Python program to:\n",
        "# ● Load the breastcancer dataset\n",
        "# ● Train an SVM Classifier with a linear kernel\n",
        "# ● Print the model's accuracy and support vectors.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform both training and testing data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train an SVM Classifier with a linear kernel on the scaled data\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the scaled test set\n",
        "y_pred = svm_model.predict(X_test_scaled)\n",
        "\n",
        "# Print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the SVM model with scaled data: {accuracy:.4f}\")\n",
        "\n",
        "# Print the support vectors\n",
        "print(f\"\\nNumber of support vectors with scaled data: {len(svm_model.support_vectors_)}\")\n",
        "# Note: Support vectors are in the scaled space\n",
        "# print(\"Support vectors with scaled data:\\n\", svm_model.support_vectors_)"
      ],
      "metadata": {
        "id": "JryYH3z2q2MA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0b5530c-2b53-4d6c-8798-352d9f890302"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the SVM model with scaled data: 0.9766\n",
            "\n",
            "Number of support vectors with scaled data: 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7:  Write a Python program to:\n",
        "# ● Load the Breast Cancer dataset\n",
        "# ● Train a Gaussian Naïve Bayes model\n",
        "# ● Print its classification report including precision, recall, and F1-score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform both training and testing data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a Gaussian Naive Bayes model on the scaled data\n",
        "gnb_model = GaussianNB()\n",
        "gnb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the scaled test set\n",
        "y_pred = gnb_model.predict(X_test_scaled)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report for Gaussian Naive Bayes model with scaled data:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAA9l7ecPrT7",
        "outputId": "a8d77b06-2482-44dd-fc08-2e3d8efb58db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Gaussian Naive Bayes model with scaled data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.90      0.91        63\n",
            "           1       0.94      0.95      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.93      0.93      0.93       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "# ● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "# C and gamma.\n",
        "# ● Print the best hyperparameters and accuracy.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform both training and testing data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the parameter grid for C and gamma\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001, 'scale', 'auto'], 'kernel': ['rbf']}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(SVC(), param_grid, refit=True, verbose=0, cv=5)\n",
        "\n",
        "# Fit the grid search to the scaled training data\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Predict on the scaled test set using the best model\n",
        "y_pred = grid_search.predict(X_test_scaled)\n",
        "\n",
        "# Print the accuracy of the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy of the best SVM model: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_NwVmwwQISh",
        "outputId": "d59d8f61-6708-48f7-ebcf-492917f49f95"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters found by GridSearchCV:\n",
            "{'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "\n",
            "Accuracy of the best SVM model: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "#● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "#sklearn.datasets.fetch_20newsgroups).\n",
        "#● Print the model's ROC-AUC score for its predictions.\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the dataset\n",
        "# We'll load a subset of the data for simplicity and faster execution\n",
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "X_train = newsgroups_train.data\n",
        "y_train = newsgroups_train.target\n",
        "X_test = newsgroups_test.data\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Multinomial Naive Bayes model\n",
        "# Multinomial Naive Bayes is suitable for text classification with count/frequency data\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_vect, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "# ROC-AUC requires probability estimates\n",
        "y_pred_proba = nb_model.predict_proba(X_test_vect)\n",
        "\n",
        "# Since ROC-AUC is typically for binary classification, we need to adapt for multiclass.\n",
        "# We can calculate the one-vs-rest (OvR) ROC-AUC score.\n",
        "# This requires the target variable to be binary or one-hot encoded for each class.\n",
        "# For simplicity, let's demonstrate for a binary case if the dataset was binary,\n",
        "# or calculate OvR AUC for each class and average.\n",
        "# fetch_20newsgroups is multiclass, so let's calculate OvR AUC.\n",
        "\n",
        "# First, we need to convert the integer labels to a format suitable for multiclass AUC (e.g., one-hot encoding)\n",
        "# However, roc_auc_score with multi_class='ovr' can handle integer labels directly if predict_proba provides columns per class.\n",
        "\n",
        "# Calculate ROC-AUC using the one-vs-rest (OvR) approach\n",
        "# Make sure the number of classes in the target is > 2 for 'ovr'\n",
        "if len(newsgroups_train.target_names) > 2:\n",
        "    # roc_auc_score with multi_class='ovr' expects y_true to be the integer labels\n",
        "    # and y_score to be the probability estimates with shape (n_samples, n_classes)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
        "    print(f\"ROC-AUC (One-vs-Rest) for Naive Bayes model: {roc_auc:.4f}\")\n",
        "else:\n",
        "    # If it were a binary dataset (e.g., 2 classes), you would use:\n",
        "    # roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1]) # assuming the positive class is at index 1\n",
        "     print(\"Dataset is not multiclass (or has less than 3 classes), OvR ROC-AUC is not directly applicable here in the standard way.\")\n",
        "     # For binary, you would calculate it like this:\n",
        "     # roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
        "     # print(f\"ROC-AUC for Naive Bayes model: {roc_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYNYYhAATFjz",
        "outputId": "c7571de2-2606-46cf-86a2-8e259715a723"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC (One-vs-Rest) for Naive Bayes model: 0.9810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0jddOuUITghw"
      }
    }
  ]
}