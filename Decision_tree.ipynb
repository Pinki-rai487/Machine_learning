{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Code: DA-AG-012\n",
        "# Decision Tree | Assignment\n",
        "\n"
      ],
      "metadata": {
        "id": "-HnWyiDYBX3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:  What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "Ans> A decision tree is a supervised learning algorithm used for classification and regression tasks. In classification, it is used to predict the class or category of a given input based on its features.\n",
        "\n",
        "1. Selecting the Best Split: The algorithm begins by selecting the best attribute to split the data at the root node. This selection is typically based on criteria like Information Gain, Gini Index, or Gain Ratio, which measure how well an attribute separates the classes.\n",
        "2. Splitting the Data: Once the best attribute is selected, the data is split into subsets based on the attribute’s values. For numerical data, this might involve dividing the data into ranges, while for categorical data, the split is based on distinct categories.\n",
        "3. Repeating the Process: The algorithm repeats the splitting process recursively for each subset, creating new internal nodes and branches. The process continues until one of the stopping criteria is met, such as when all data points in a node belong to the same class or when further splitting does not significantly improve the classification.\n",
        "4. Assigning Class Labels: Once the splitting process is complete, the leaf nodes are assigned class labels based on the majority class in each subset. These labels represent the final decisions or classifications made by the tree.\n",
        "5. Pruning the Tree: To improve generalization and prevent overfitting, the tree is pruned by removing nodes that do not contribute significantly to classification accuracy. Pruning can be performed using techniques such as Reduced Error Pruning or Cost-Complexity Pruning."
      ],
      "metadata": {
        "id": "gc-07jcpBzU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "**Gini Impurity**: Gini Impurity is a measure of the likelihood of an incorrect classification of a new instance of a random variable if that instance was randomly labeled according to the distribution of labels in the dataset. It quantifies the degree of impurity or disorder in a dataset.\n",
        "\n",
        "**Entropy**: Entropy, rooted in information theory, measures the amount of uncertainty or randomness in a dataset. In the context of decision trees, it quantifies the homogeneity of a dataset.\n",
        "\n",
        "Impact on tree: Choice affects which splits are selected, the sensitivity to minority classes, the tree shape, and subtle differences in final predictions, but in most practical scenarios, both metrics perform comparably."
      ],
      "metadata": {
        "id": "iqouVh4IBzfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "Ans>\n",
        "**Pre-pruning**:\n",
        "1. Pre-Pruning, also known as early stopping, involves halting the construction of a decision tree before it fully fits the training data.\n",
        "2. Risk of overfitting is lower if stopping criteria are well tuned.\n",
        "3. Prevents large trees proactively\n",
        "4. practical advantage: re-pruning is often used when computational resources are limited or datasets are large.\n",
        "\n",
        "**Post-pruning**:\n",
        "1. ost-Pruning, also called backward pruning, allows the tree to grow completely and then removes branches that contribute little to predictive performance.\n",
        "2. Risk of overfitting is controlled by pruning irrelevant branches.\n",
        "3. Reduces complexity after seeing full growth\n",
        "4. practical use: when prediction accuracy is critical and sufficient data is available for validation."
      ],
      "metadata": {
        "id": "hD6XvxT_BzkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "Ans> Information Gain (IG) is a measure used in decision trees to quantify the effectiveness of a feature in splitting the dataset into classes. It calculates the reduction in entropy (uncertainty) of the target variable (class labels) when a particular feature is known.\n",
        "nformation Gain helps us understand how much a particular feature contributes to making accurate predictions in a decision tree. Features with higher Information Gain are considered more informative and are preferred for splitting the dataset, as they lead to nodes with more homogenous classes."
      ],
      "metadata": {
        "id": "fD2tmBFsBznr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "Ans> Common Real-World Applications\n",
        "1. Finance and Banking: They help in credit scoring, loan approval predictions, fraud detection, and risk assessment, as they can handle both numerical and categorical data.\n",
        "2. Marketing and Sales: Businesses use Decision Trees to segment customers, predict churn, recommend products, and optimize marketing strategies based on customer behavior.\n",
        "3. Operations and Manufacturing: Used for quality control, predictive maintenance, and process optimization by analyzing machinery sensor data and production metrics.\n",
        "\n",
        "Advantages:\n",
        "* Interpretability: The tree structure is easy to visualize and understand, making results transparent and explainable.\n",
        "* Non-parametric: Decision Trees do not assume any probability distributions, making them flexible for diverse datasets.\n",
        "* Handles mixed data types: Can manage numerical, categorical, and missing data without extensive preprocessing.\n",
        "\n",
        "Limitations:\n",
        "* Overfitting: Decision Trees can easily overfit especially with deep trees and noisy data, reducing generalization to new datasets.\n",
        "* Lack of smooth predictions: In regression, predictions are piecewise constant rather than smooth, which may be inappropriate for certain continuous outputs."
      ],
      "metadata": {
        "id": "U956OprEBzrF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZymME__BXY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cc84736-8991-4b18-b04b-95daf9c6cedc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ],
      "source": [
        "# Question 6:   Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier using the Gini criterion\n",
        "# ● Print the model’s accuracy and feature importances\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier using the Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7:  Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "# a fully-grown tree\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with max_depth=3\n",
        "clf_limited_depth = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_limited_depth.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_limited_depth = clf_limited_depth.predict(X_test)\n",
        "\n",
        "# Calculate and print the model's accuracy for the limited depth tree\n",
        "accuracy_limited_depth = accuracy_score(y_test, y_pred_limited_depth)\n",
        "print(f\"Model Accuracy (max_depth=3): {accuracy_limited_depth:.2f}\")\n",
        "\n",
        "# Train a fully-grown Decision Tree Classifier (default behavior)\n",
        "clf_full_depth = DecisionTreeClassifier(random_state=42)\n",
        "clf_full_depth.fit(X_train, y_train) # This should be y_train, not y_test\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_full_depth = clf_full_depth.predict(X_test)\n",
        "\n",
        "# Calculate and print the model's accuracy for the fully-grown tree\n",
        "accuracy_full_depth = accuracy_score(y_test, y_pred_full_depth)\n",
        "print(f\"Model Accuracy (fully-grown): {accuracy_full_depth:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9icPHZ1d4sG",
        "outputId": "5da8b6a2-dd8f-4a3a-918b-e79977ab5944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy (max_depth=3): 1.00\n",
            "Model Accuracy (fully-grown): 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "# ● Load the California Housing dataset from sklearn\n",
        "# ● Train a Decision Tree Regressor\n",
        "# ● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate and print the Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(housing.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq0iWPSBVZ79",
        "outputId": "cf9606a1-8460-49ee-9129-5de623c46685"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.50\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "# ● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid to tune\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Perform GridSearchCV to find the best parameters\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Get the best model\n",
        "best_clf = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred = best_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with best parameters: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIVt_T7JW7y_",
        "outputId": "b6485b63-3aa7-4be0-ec2a-8772132bdcd6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy with best parameters: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "\n",
        "Ans>\n",
        "1. Handling Missing Values\n",
        "In healthcare datasets, missing values are common due to incomplete records, patient non-responses, or data entry errors. Handling them is crucial for model accuracy.\n",
        "Steps:\n",
        "* Data Exploration: Identify missing values by column and calculate the proportion of missing entries. Visualizations like heatmaps or summary tables can help detect patterns.\n",
        "* Assess Mechanism of Missingness: Classify missing data as MCAR (Missing Completely at Random), MAR (Missing at Random), or MNAR (Missing Not at Random). This informs the imputation strategy.\n",
        "* Imputation Strategy:\n",
        "Numerical Features: Fill missing values using mean, median, or K-nearest neighbors (KNN) imputation, depending on distribution skewness.\n",
        "Categorical Features: Use mode imputation or create a separate 'missing' category.\n",
        "2. Encoding Categorical Features\n",
        "Decision trees can handle categorical data natively in some implementations, but many libraries require numeric encoding.\n",
        "Nominal Features: One-hot encoding is standard to avoid implicit ordering. Target encoding can be used cautiously if data leakage is prevented.\n",
        "Integration: Combine encoded features with numerical data, ensuring feature scaling is not required for decision trees.\n",
        "4. Train a Decision Tree Model:\n",
        "Decision trees recursively partition the feature space based on splitting criteria like Gini impurity or entropy.\n",
        "\n",
        "* Split Dataset: Use train-test split (e.g., 80%-20%) or cross-validation frameworks.\n",
        "* Model Initialization: Choose parameters like criterion, max_depth, min_samples_split, and min_samples_leaf for initial trials.\n",
        "* Training: Fit the tree on the training data using the features and the target variable (disease presence: yes/no).\n",
        "\n",
        "5. Hyperparameter Tuning\n",
        "To improve generalization and prevent overfitting:\n",
        "Key Hyperparameters:\n",
        "* max_depth: Restricts tree depth.\n",
        "* min_samples_split and min_samples_leaf: Avoid splits with very few records.\n",
        "* max_features: Controls the number of features considered at each split.\n",
        "* criterion: 'gini' or 'entropy' for classification quality.\n",
        "\n",
        "5. Evaluating Model Performance\n",
        "Steps:\n",
        "* Confusion Matrix: Calculate TP, FP, FN, TN to derive precision, recall, and F1-score.\n",
        "* ROC Curve / AUC: Assess the model’s discriminative ability.\n",
        "* Cross-Validation Scores: Ensure stability across folds.\n",
        "* Calibration: Verify predicted probabilities align with actual disease occurrence if probability outputs are relevant.\n",
        "\n",
        "\n",
        "Business Value in Real-World Healthcare\n",
        "* Early Disease Detection: Predictive insights allow proactive intervention, potentially reducing hospitalizations and improving patient outcomes.\n",
        "Resource Allocation: Prioritize high-risk patients for further tests or monitoring.\n",
        "* Cost Savings: Reduce unnecessary tests for low-risk patients and optimize operational efficiency.\n",
        "* Personalized Care: Inform treatment plans based on model predictions, enabling targeted preventative measures.\n",
        "* Compliance & Reporting: Support evidence-based recommendations for healthcare policies and insurance purposes."
      ],
      "metadata": {
        "id": "JmBdRgAQY-N3"
      }
    }
  ]
}